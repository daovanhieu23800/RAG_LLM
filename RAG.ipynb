{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 0}, page_content='AI VIET NAM – AI COURSE 2024\\nKiểm tra tuân thủ đội mũ bảo vệ với YOLOv10\\nDinh-Thang Duong, Quang-Vinh Dinh\\nNgày 22 tháng 6 năm 2024\\nI. Giới thiệu\\nObject Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh\\nvực Computer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng\\ntrong một tấm ảnh. Đây là một trong những bài toán quan trọng và phức tạp trong Computer\\nVision, với ứng dụng rộng rãi từ nhận diện khuôn mặt, nhận dạng biển số xe, đến theo dõi đối\\ntượng trong video và tự động lái xe.\\nHình 1: Chương trình phát hiện đối tượng có mang mũ bảo hiểm.\\nTrong project này, chúng ta sẽ xây dựng một chương trình phát hiện các nhân viên có đeo mũ\\nbảo vệ trong công trường hay không? Mô hình mà chúng ta sử dụng sẽ là mô hình YOLOv10.\\nTheo đó, Input và Output của chương trình là:\\n•Input:Một tấm ảnh.\\n•Output: Tọa độ (bounding box) của các nhân viên và phần mũ bảo hiểm.\\nTổng quan, các bước thực hiện trong project của chúng để hoàn thiện hệ thống Helmet Safety\\nDetection bao gồm:\\n1'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 1}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 2: Tổng quan các bước thực hiện trong project.\\nLuồng xử lý (pipeline) của chương trình Helmet Safety Detection mà chúng ta sẽ xây dựng có\\ndạng như sau:\\nHình 3: Pipeline của chương trình.\\n2'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 2}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nII. Cài đặt chương trình\\nTrong phần này, chúng ta sẽ tìm hiểu cách sử dụng YOLOv10 bao gồm việc sử dụng pre-trained\\nmodel và huấn luyện (fine-tuning) YOLOv10 trên bộ dữ liệu Helmet Safety Detection.\\nII.I. Chuẩn bị môi trường lập trình\\nĐể thuận tiện trong việc sử dụng YOLOv10, chúng ta sẽ dùng Google Colab làm môi trường cài\\nđặt và thực thi code. Các bước sử dụng Google Colab được thực hiện như sau (nếu các bạn đã\\nbiết cách kích hoạt GPU cho Google Colab thì có thể bỏ qua phần này):\\n•Bước 1: Truy cập vào đường dẫn sau: link. Nếu truy cập thành công, các bạn sẽ thấy giao\\ndiện như hình dưới đây:\\nHình 4: Giao diện chính của Google Colab\\nSau đó, các bạn hãy đăng nhập bằng tài khoản Google của mình. Nếu đăng nhập thành\\ncông, một cửa sổ mới sẽ hiện lên như hình dưới đây:\\n3'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 3}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 5: Giao diện Google Colab sau khi đăng nhập thành công\\n•Bước 2: Khởi tạo notebook mới. Notebook sẽ là giao diện để ta có thể viết các dòng lệnh\\nPython. Để tạo được notebook, các bạn thực hiện thao tác theo hình dưới đây:\\nHình 6: Khởi tạo notebook mới\\n•Bước 3: Thay đổi runtime của notebook từ CPU thành GPU.\\n4'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 4}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 7: Các bước kích hoạt GPU cho notebook mới trên Google Colab\\n•Bước 4: Cuối cùng, để có thể thực thi các dòng lệnh Python, ta cần khởi động notebook.\\nCác thao tác khởi động một notebook trong Google Colab sẽ như hình sau:\\nHình 8: Khởi động một notebook có GPU trong Google Colab\\nSau khi thực hiện các bước trên, các bạn đã có một môi trường code Python với GPU miễn phí\\ntừ Google.\\n5'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 5}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nII.II. Cài đặt và sử dụng pre-trained model\\nMột cách nhanh chóng để sử dụng được YOLOv10 đó là sử dụng pre-trained model (mô hình đã\\nđược huấn luyện sẵn trên bộ dữ liệu COCO - một bộ dữ liệu rất lớn). Để sử dụng pre-trained\\nmodel, các bạn làm như sau:\\n1.Tải mã nguồn YOLOv10 từ GitHub: Để sử dụng YOLOv10, chúng ta cần tải mã\\nnguồn (source code) của YOLOv10 về môi trường cài đặt code, mã nguồn của YOLOv10\\nđược công khai trên GitHub. Như vậy, các bạn sẽ thực hiện theo các bước sau:\\n•Bước 1: Các bạn truy cập vào đường dẫn GitHub của YOLOv10 tại đây. Nếu truy\\ncập thành công, các bạn sẽ thấy giao diện như hình dưới:\\nHình 9: Giao diện GitHub của YOLOv10\\n•Bước 2: Tại trang GitHub của YOLOv10, các bạn chọn mục Code(có màu nền xanh\\ndương như ảnh dưới) và chọn nút copy đường dẫn như ảnh minh họa dưới đây:\\n6'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 6}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 10: Copy đường dẫn để tải mã nguồn YOLOv10\\n•Bước 3: Quay lại Google Colab, các bạn khởi tạo một code cell sử dụng lệnh:\\n!git clone <link> (trong đó <link> là đường dẫn GitHub đã copy ở bước 2).\\nNếu code cell thực thi thành công, ta kết quả sẽ hiển thị như hình sau:\\nHình 11: Tải mã nguồn YOLOv10 sử dụng lệnh git clone\\nĐể kiểm tra, các bạn có thể refresh lại phần Filescủa Google Colab để xem thư mục\\nYOLOv10 đã xuất hiện hay chưa.\\n7'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 7}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 12: Thư mục YOLOv10\\n2.Cài đặt các thư viện cần thiết: Mã nguồn YOLOv10 được xây dựng bằng rất nhiều\\ncác thư viện Python khác nhau. Vì vậy, để có thể chạy được YOLOv10, ta cần tải các gói\\nthư viện cần thiết mà YOLOv10 yêu cầu bằng cách sử dụng file setup có sẵn trong mã\\nnguồn, các bạn có thể làm như sau:\\n1%cd yolov10\\n2!pip install -q -r requirements .txt\\n3!pip install -e .\\n3.Tải trọng số của pre-trained models: Các bạn tải file pretrained model tại đây và đặt\\nfile đã tải vào thư mục ./yolov10 . Trên Google Colab, việc này có thể được thực hiện\\nthông qua lệnh wgetnhư hình dưới đây:\\n8'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 8}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 13: Tải pretrained model sử dụng lệnh wget\\nĐể kiểm tra xem file pretrained model đã được tải về thành công hay chưa, các bạn refresh\\nphầnFilescủa Google Colab và tìm kiếm file có tên YOLOv10n.pt :\\nHình 14: File pretrained model\\n9'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 9}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n4.Khởi tạo mô hình: Để khởi tạo mô hình với trọng số vừa tải về, các bạn chạy đoạn code\\nsau:\\n1from ultralytics import YOLOv10\\n2\\n3MODEL_PATH = ’yolov10n .pt ’\\n4model = YOLOv10 ( MODEL_PATH )\\n5.Tải ảnh cần dự đoán: Chúng ta sẽ test mô hình trên một ảnh bất kì. Các bạn có thể tự\\nchọn ảnh của riêng mình hoặc sử dụng ảnh tại đây. Các bạn có thể chạy đoạn code sau để\\ntải ảnh này vào colab tự động:\\n1! gdown ’1 tr9PSRRdlC2pNir7jsYugpSMG -7 v32VJ ’ -O ’./ images /’\\n6.Dự đoán: Để chạy dự đoán cho ảnh đã tải về, các bạn truyền đường dẫn ảnh vào mô hình\\nnhư đoạn code sau:\\n1IMG_PATH = ’./ images / HCMC_Street .jpg ’\\n2result = model ( source = IMG_PATH )[0]\\nHình 15: Ảnh cần dự đoán.\\n7.Lưu kết quả dự đoán: Để lưu lại ảnh đã được dự đoán, các bạn chạy đoạn code sau:\\n1result . save (’./ images / HCMC_Street_predict .png ’)\\n10'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 10}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 16: Kết quả dự đoạn của mô hình YOLOv10 phiên bản nano (yolov10n.pt) .\\n8.Dự đoán youtube video: Bên cạnh source về ảnh, các bạn cũng có thể input source với\\ncác tham số khác nhau, đại diện cho các dữ liệu đầu vòa khác nhau:\\nHình 17: Tổng hợp các kiểu dữ liệu đầu vào YOLOv10 hỗ trợ trong việc thực hiện predict.\\nVí dụ, để dự đoán với input là youtube video, các bạn chỉ cần thay thế IMG_PATH\\n11'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 11}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nbằng đường dẫn youtube video như đoạn code sau:\\n1YOUTUBE_VIDEO_PATH = ’https :// youtu .be/ wqPSsu7XQ74 ’\\n2video_result = model ( source = YOUTUBE_VIDEO_PATH )\\nKết quả dự đoán sẽ là một video được lưu dưới dạng .avi trong thư mục: /content/yolov10\\n/runs/detect/predict\\nII.III. Huấn luyện YOLOv10 trên tập dữ liệu mới\\nTrong phần này, chúng ta sẽ thực hiện huấn luyện mô hình YOLOv10 (fine-tuning) trên bộ dữ\\nliệu Helmet Safety Detection. Để tránh sự nhầm lẫn, phần này sẽ được thực hiện ở một file colab\\nkhác so với phần trước. Các bước thực hiện như sau:\\n1.Tải bộ dữ liệu: Chúng ta sẽ giải quyết bài toán phát hiện các công nhân. Bộ dữ liệu được\\nsử dụng trong bài toán này là Helmet Safety. Để dễ hình dung, các bạn có thể quan sát\\nảnh minh họa sau:\\nHình 18: Một vài mẫu dữ liệu trong bộ dữ liệu về Helmet Safety Detection.\\nĐể tải bộ dữ liệu trên, các bạn hãy chạy đoạn code sau (link tải bộ dữ liệu tại đây):\\n1! gdown ’1 twdtZEfcw4ghSZIiPDypJurZnNXzMO7R ’\\nGiải nén bộ dữ liệu vào folder datasets . Các bạn thực thi đoạn code sau:\\n1! gdown ’1 twdtZEfcw4ghSZIiPDypJurZnNXzMO7R ’\\n2! mkdir safety_helmet_dataset\\n3! unzip -q ’/ content / Safety_Helmet_Dataset .zip ’ -d ’/ content /\\nsafety_helmet_dataset ’\\n12'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 12}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 19: Thư mục chứa bộ dữ liệu\\nQuan sát thư mục, có thể thấy bộ dữ liệu này đã được gán nhãn và đưa vào format cấu\\ntrúc dữ liệu training theo yêu cầu của YOLO. Vì vậy, chúng ta sẽ không cần thực hiện\\nbước chuẩn bị dữ liệu ở bài này.\\n2.Cài đặt và import các thư viện cần thiết: Tương tự như phần trước, các bạn chạy\\ncác đoạn code sau để cài đặt các gói thư viện để sử dụng được YOLOv10:\\n1!git clone https :// github .com/THU -MIG/ yolov10 .git\\n2%cd yolov10\\n3!pip install -q -r requirements .txt\\n4!pip install -e .\\n3.Khởi tạo mô hình YOLOv10: Chúng ta sẽ khởi tạo mộ hình YOLOv10 với phiên\\nbảnnano (n) từ trọng số đã được huấn luyện trên bộ dữ liệu COCO. Để tải trọng số\\nyolov10n.pt, các bạn chạy đoạn code sau:\\n1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\\nyolov10n .pt\\nSau đó, để khởi tạo mô hình từ trọng số đã tải về, các bạn chạy đoạn code sau:\\n13'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 13}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n1from ultralytics import YOLOv10\\n2\\n3MODEL_PATH = ’yolov10n .pt ’\\n4model = YOLOv10 ( MODEL_PATH )\\n4.Huấn luyện mô hình: Chúng ta tiến hành huấn luyện YOLOv10 trên bộ dữ liệu Helmet\\nSafety Detection với 50 epochs và kích thước ảnh là 640. Các bạn chạy đoạn code sau:\\n1YAML_PATH = ’../ safety_helmet_dataset / data . yaml ’\\n2EPOCHS = 50\\n3IMG_SIZE = 640\\n4BATCH_SIZE = 256\\n5\\n6model . train ( data = YAML_PATH ,\\n7 epochs =EPOCHS ,\\n8 batch = BATCH_SIZE ,\\n9 imgsz = IMG_SIZE )\\nHình 20: Quá trình huấn luyện mô mình YOLOv10.\\n5.Đánh giá mô hình: Để thực hiện đánh giá mô hình trên tập test, các bạn chạy đoạn code\\nsau:\\n14'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 14}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n1TRAINED_MODEL_PATH = ’runs / detect / train / weights / best .pt ’\\n2model = YOLOv10 ( TRAINED_MODEL_PATH )\\n3\\n4model .val ( data = YAML_PATH ,\\n5 imgsz = IMG_SIZE ,\\n6 split =’test ’)\\nHình 21: Đánh giá mô hình sau khi huấn luyện trên tập test.\\n6.OPTIONAL : Phần này sẽ bàn luận thêm một vài vấn đề trong YOLOv10 bao gồm một\\nvài các tham số trong lệnh training, lệnh đánh giá mô hình và gán nhãn dữ liệu.\\n•Các tham số trong lệnh training: Dòng lệnh training tại bước 4 có mặc định sẵn\\nmột vài tham số, các tham số này các bạn có thể tùy chỉnh theo ý muốn của mình,\\nvới một số tham số có giá trị khác nhau sẽ cho ra hiệu suất mô hình khác nhau. Sau\\nđây là ý nghĩa cơ bản của một vài tham số trên:\\n– img:Kích thước ảnh training, các ảnh train và test sẽ được resize lại về kích\\nthước bạn gán, mặc định là 640. Các bạn hoàn toàn có thể thử nghiệm trên các\\nkích thước ảnh khác nhau.\\n– batch: Khi thực hiện tính toán trong quá trình training, các mô hình có thể đọc\\nmột lúc toàn bộ dữ liệu train hoặc chia ra đọc theo từng batch. Với mặc định là\\n64, bộ dữ liệu train sẽ được chia ra thành các batch có 64 mẫu dữ liệu. Các bạn\\ncó thể cài đặt các giá trị khác theo 2n(n≥0).\\n– epochs: Số lần lặp qua bộ dữ liệu trong quá trình huấn luyện.\\n– data: Thông tin bộ dữ liệu (file .yaml) mà bạn mong muốn training.\\n– weights: File pretrained model sử dụng. Các bạn có thể tải và sử dụng các file\\npretrained model khác nhau trong danh sách này.\\n•Gán nhãn dữ liệu: Vì YOLOv10 là học có giám sát (supervised learning), tức các\\nmẫu trong bộ dữ liệu training cần phải có labels tương ứng với từng mẫu. Vì vậy, để\\ncó thể cung cấp thêm dữ liệu hoặc tạo ra một bộ dữ liệu mới, với các class mới. Ta cần\\nphải thực hiện gán nhãn dữ liệu, việc gán nhãn này sẽ phải thực hiện thủ công. Trong\\nbài hướng dẫn này, chúng ta sẽ tìm hiểu cách sử dụng labelImg. Để cài đặt labelImg\\n15'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 15}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nvề máy tính, có rất nhiều cách khác nhau (các bạn có thể đọc file README.md trên\\ntrang GitHub của labelImg), ở đây chúng ta sẽ chọn cách cài đặt với Anaconda:\\n– Bước 1: Tải mã nguồn của labelImg tại trang GitHub sử dụng dòng lệnh sau\\n(các dòng lệnh trong các bước ở đây sẽ được thực hiện trong cmd/terminal):\\n1$ git clone https :// github . com/ heartexlabs / labelImg . git\\n– Bước 2: Cài đặt Anaconda tại đây.\\n– Bước 3: Sau khi đã cài đặt Anaconda, các bạn sẽ khởi tạo môi trường Anaconda\\nbằng dòng lệnh sau:\\n1$ conda create -n labelimg_env python =3.9 -y\\n– Bước 4: Kích hoạt môi trường ảo đã tạo ở bước 3 sử dụng lệnh:\\n1$ conda activate labelimg_env\\n– Bước 5: Di chuyển vào thư mục mã nguồn của labelImg sử dụng lệnh sau:\\n1$ cd < path_to_labelImg_folder >\\n– Bước 6: Thực hiện lần lượt các dòng lệnh:\\n1$ conda install pyqt =5\\n2$ conda install -c anaconda lxml\\n3$ pyrcc5 -o libs / resources .py resources .qrc\\n– Bước 7: Khởi động giao diện của labelImg sử dụng lệnh:\\n1$ python labelImg .py\\nNếu thành công, các bạn sẽ thấy một giao hiện màn hình như sau:\\n16'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 16}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nVề sau, mỗi lần cần sử dụng labelImg, ta chỉ cần kích hoạt môi trường đã tạo này và\\nkhởi động labelImg (tức chỉ thực hiện bước 4 và bước 7).\\nBây giờ, giả sử ta muốn gán nhãn một tập ảnh người (class person). Chúng ta sẽ thực\\nhiện các bước như sau (chuẩn bị sẵn một thư mục hình ảnh):\\n– Bước 1: Truy cập vào thư mục data trong mã nguồn labelImg, sửa lại nội dung\\nfilepredefined_classes.txt bằng tên của các class trong bộ dữ liệu của bạn,\\nmỗi tên class sẽ là một hàng trong file:\\nHình 22: Nội dung trong file predefined_classes.txt . Mỗi hàng trong file tương ứng với mỗi\\ntên class trong bộ dữ liệu.\\nCác bạn sẽ sửa lại nội dung file bằng tên của class trong bộ dữ liệu của mình, giả\\nsử chỉ có class là person, vậy ta chỉ sửa lại file thành như sau:\\nHình 23: Nội dung trong file predefined_classes.txt sau khi cập nhật.\\nĐối với bộ dữ liệu Helmet Safety Dataset, ta sẽ có 3 class gồm (’head’, ’helmet’,\\n’person’), các bạn có thể coi ở trong file data.yaml ở thư mục bộ dữ liệu:\\nHình 24: Nội dung file .yaml trong data Helmet Safety Detection.\\n– Bước 2: Truy cập vào giao diện labelImg, các bạn hãy làm theo các bước theo\\nhình sau:\\n17'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 17}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 25: Mở thư mục chứa ảnh cần gán nhãn và chuyển format nhãn thành YOLO\\n– Bước 3: Nếu thành công, các bạn sẽ thấy giao diện như sau:\\nHình 26: Giao diện sau khi chọn thư mục ảnh cần gán nhãn\\n– Bước 4: Thực hiện gán nhãn. Các bạn làm theo hình dưới đây:\\n18'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 18}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 27: Quá trình gán nhãn\\nSau khi kết thúc gán nhãn cho một ảnh, các bạn save lại (CTRL + S) và di\\nchuyển đi hình tiếp theo (phím D (Next Image) hoặc phím A (Prev Image)). Khi\\nđã gán nhãn hết tất cả các ảnh, các bạn có thể kiểm tra tại thư mục chứa ảnh\\ncủa mình, nếu thấy các file .txt cho từng ảnh bạn đã gán nhãn, bạn đã gán nhãn\\nthành công:\\nHình 28: Thư mục chứa ảnh sau khi gán nhãn\\nĐây là công việc tưởng chừng nhẹ nhàng nhưng lại hết sức quan trọng, vì nó sẽ\\nảnh hưởng rất lớn đến kết quả từ mô hình của bạn, vì vậy cần thật sự tập trung\\nvà cẩn trọng trong việc gán nhãn dữ liệu.\\n19'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 19}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nIII. Câu hỏi trắc nghiệm\\n1. Trong một chương trình Helmet Safety Detection với chức năng dùng để phát hiện người\\ncó đội mũ bảo vệ hay không trong một ảnh, Input của chương trình là gì?\\n(a) Mô hình Object Detection.\\n(b) Bounding Box.\\n(c) Ảnh bất kì.\\n(d) Ảnh chứa người.\\n2. Trong một chương trình Object Detection, Output của mô hình Object Detection là gì?\\n(a) Ảnh chứa người.\\n(b) Tọa độ bounding box nếu có.\\n(c) Tọa độ bounding box và tên class nếu có.\\n(d) Ảnh chứa bounding box nếu có.\\n3. Trong các bước cài đặt và thực hiện huấn luyện YOLOv10, bước nào sau đây không nằm\\ntrong quy trình?\\n(a) Tải pretrained model.\\n(b) Cài đặt hàm huấn luyện.\\n(c) Cài đặt thư viện ultralytics.\\n(d) Tải bộ dữ liệu.\\n4. Trong các bước sử dụng labelImg để gán nhãn dữ liệu huấn luyện YOLOv10, bước nào sau\\nđây không nằm trong thao tác chính?\\n(a) Đổi format label sang PascalVOC.\\n(b) Chọn thư mục chứa ảnh.\\n(c) Chọn thư mục lưu label.\\n(d) Sửa tên class trong predefine_classes.txt\\n5. Trong file cấu hình của bộ dữ liệu (file .yaml), trường thông tin nccó ý nghĩa gì?\\n(a) Số mẫu dữ liệu trong dataset.\\n(b) Số lượng dataset.\\n(c) Số thứ tự của dataset.\\n(d) Số lượng class.\\n6. Lệnh nào sau đây dùng để kích hoạt môi trường ảo tên yolo_env trong conda?\\n(a)conda deactivate yolo_env\\n(b)conda env deactivate yolo_env\\n20'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 20}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n(c)conda env activate yolo_env\\n(d)conda activate yolo_env\\n7. Cho hình ảnh cấu trúc cây thư mục (trong Google Colab) và đoạn lệnh sau:\\n1YAML_PATH = ’./ safety_helmet_dataset / data . yaml ’\\n2EPOCHS = 50\\n3IMG_SIZE = 640\\n4\\n5model . train ( data = YAML_PATH ,\\n6 epochs =EPOCHS ,\\n7 imgsz = IMG_SIZE )\\nGiả sử các bạn đang ở thư mục /content/yolov10 , khi thực thi đoạn lệnh trên có xảy ra\\nlỗi không?\\n21'),\n",
       " Document(metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 21}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n(a) Có.\\n(b) Không.\\n8. Trong câu lệnh huấn luyện mô hình, tham số EPOCHS có ý nghĩa là?\\n(a) Tham số mô hình quan trọng trong quá trình huấn luyện.\\n(b) Được sử dụng để chỉ định số lần lặp lại quá trình huấn luyện.\\n(c) Một biến số ngẫu nhiên trong quá trình huấn luyện mô hình.\\n(d) Số lần mà toàn bộ dữ liệu huấn luyện sẽ được sử dụng để cập nhật các trọng số của\\nmô hình.\\n9. Trong câu lệnh huấn luyện mô hình, tham số BATCH_SIZE có ý nghĩa là?\\n(a) Số lần lặp lại quá trình huấn luyện mô hình.\\n(b) Số lượng tham số của mô hình cần được cập nhật sau mỗi lượt huấn luyện.\\n(c) Số lượng mẫu dữ liệu được sử dụng trong mỗi lần cập nhật gradient.\\n(d) Thời gian tối đa cho mỗi lần huấn luyện mô hình.\\n10. Trong ngữ cảnh của project này, việc huấn luyện (training) mô hình còn được gọi với từ\\nkhóa là\"fine-tuning\" , thuật ngữ này có thể được hiểu như thế nào?\\n(a) Quá trình điều chỉnh lại mô hình đã được huấn luyện trước đó trên một tập dữ liệu\\nmới để cải thiện hiệu suất hoặc thích nghi với một tác vụ cụ thể.\\n(b) Quá trình huấn luyện mô hình từ đầu mà không sử dụng bất kỳ trọng số nào từ mô\\nhình đã được huấn luyện trước đó.\\n(c) Quá trình điều chỉnh siêu tham số của mô hình để tối ưu hóa độ chính xác.\\n(d) Quá trình lấy mẫu một cách ngẫu nhiên từ tập dữ liệu huấn luyện để đảm bảo tính\\nngẫu nhiên và đại diện của dữ liệu.\\n- Hết -\\n22')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loader = PyPDFLoader\n",
    "FILE_PATH = \"./AIO2024_Module01_Project_YOLOv10_Description.pdf\"\n",
    "loader = Loader(FILE_PATH)\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x256849677f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub-documents: 29\n",
      "page_content='AI VIET NAM – AI COURSE 2024\n",
      "Kiểm tra tuân thủ đội mũ bảo vệ với YOLOv10\n",
      "Dinh-Thang Duong, Quang-Vinh Dinh\n",
      "Ngày 22 tháng 6 năm 2024\n",
      "I. Giới thiệu\n",
      "Object Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh\n",
      "vực Computer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng\n",
      "trong một tấm ảnh. Đây là một trong những bài toán quan trọng và phức tạp trong Computer\n",
      "Vision, với ứng dụng rộng rãi từ nhận diện khuôn mặt, nhận dạng biển số xe, đến theo dõi đối\n",
      "tượng trong video và tự động lái xe.\n",
      "Hình 1: Chương trình phát hiện đối tượng có mang mũ bảo hiểm.\n",
      "Trong project này, chúng ta sẽ xây dựng một chương trình phát hiện các nhân viên có đeo mũ\n",
      "bảo vệ trong công trường hay không? Mô hình mà chúng ta sử dụng sẽ là mô hình YOLOv10.\n",
      "Theo đó, Input và Output của chương trình là:\n",
      "•Input:Một tấm ảnh.\n",
      "•Output: Tọa độ (bounding box) của các nhân viên và phần mũ bảo hiểm.' metadata={'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Number of sub-documents:\", len(docs))\n",
    "print(docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "retriever = vector_db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant documents: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 12, 'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf'}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 19: Thư mục chứa bộ dữ liệu\\nQuan sát thư mục, có thể thấy bộ dữ liệu này đã được gán nhãn và đưa vào format cấu\\ntrúc dữ liệu training theo yêu cầu của YOLO. Vì vậy, chúng ta sẽ không cần thực hiện\\nbước chuẩn bị dữ liệu ở bài này.\\n2.Cài đặt và import các thư viện cần thiết: Tương tự như phần trước, các bạn chạy\\ncác đoạn code sau để cài đặt các gói thư viện để sử dụng được YOLOv10:\\n1!git clone https :// github .com/THU -MIG/ yolov10 .git\\n2%cd yolov10\\n3!pip install -q -r requirements .txt\\n4!pip install -e .\\n3.Khởi tạo mô hình YOLOv10: Chúng ta sẽ khởi tạo mộ hình YOLOv10 với phiên\\nbảnnano (n) từ trọng số đã được huấn luyện trên bộ dữ liệu COCO. Để tải trọng số\\nyolov10n.pt, các bạn chạy đoạn code sau:\\n1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\\nyolov10n .pt\\nSau đó, để khởi tạo mô hình từ trọng số đã tải về, các bạn chạy đoạn code sau:\\n13'),\n",
       " Document(metadata={'page': 20, 'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf'}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n(c)conda env activate yolo_env\\n(d)conda activate yolo_env\\n7. Cho hình ảnh cấu trúc cây thư mục (trong Google Colab) và đoạn lệnh sau:\\n1YAML_PATH = ’./ safety_helmet_dataset / data . yaml ’\\n2EPOCHS = 50\\n3IMG_SIZE = 640\\n4\\n5model . train ( data = YAML_PATH ,\\n6 epochs =EPOCHS ,\\n7 imgsz = IMG_SIZE )\\nGiả sử các bạn đang ở thư mục /content/yolov10 , khi thực thi đoạn lệnh trên có xảy ra\\nlỗi không?\\n21'),\n",
       " Document(metadata={'page': 7, 'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf'}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 12: Thư mục YOLOv10\\n2.Cài đặt các thư viện cần thiết: Mã nguồn YOLOv10 được xây dựng bằng rất nhiều\\ncác thư viện Python khác nhau. Vì vậy, để có thể chạy được YOLOv10, ta cần tải các gói\\nthư viện cần thiết mà YOLOv10 yêu cầu bằng cách sử dụng file setup có sẵn trong mã\\nnguồn, các bạn có thể làm như sau:\\n1%cd yolov10\\n2!pip install -q -r requirements .txt\\n3!pip install -e .\\n3.Tải trọng số của pre-trained models: Các bạn tải file pretrained model tại đây và đặt\\nfile đã tải vào thư mục ./yolov10 . Trên Google Colab, việc này có thể được thực hiện\\nthông qua lệnh wgetnhư hình dưới đây:\\n8'),\n",
       " Document(metadata={'page': 10, 'source': './AIO2024_Module01_Project_YOLOv10_Description.pdf'}, page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nHình 16: Kết quả dự đoạn của mô hình YOLOv10 phiên bản nano (yolov10n.pt) .\\n8.Dự đoán youtube video: Bên cạnh source về ảnh, các bạn cũng có thể input source với\\ncác tham số khác nhau, đại diện cho các dữ liệu đầu vòa khác nhau:\\nHình 17: Tổng hợp các kiểu dữ liệu đầu vào YOLOv10 hỗ trợ trong việc thực hiện predict.\\nVí dụ, để dự đoán với input là youtube video, các bạn chỉ cần thay thế IMG_PATH\\n11')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke(\"What is YOLO?\")\n",
    "\n",
    "print(\"Number of relevant documents:\", len(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=nf4_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=model_pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 là một phiên bản nano của mô hình dự đoán ảnh YOLOv10, được tạo bằng cách tải trọng số của mô hình được huấn luyện trên dữ liệu COCO và khởi tạo tại Google Colab bằng cách cài đặt các thư viện cần thiết và tải trọng số của pre-trained models.\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "USER_QUESTION = \"YOLOv10 là gì?\"\n",
    "output = rag_chain.invoke(USER_QUESTION)\n",
    "answer = output.split('Answer:')[1].strip()\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
